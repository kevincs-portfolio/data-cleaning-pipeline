# -*- coding: utf-8 -*-
"""Data_Cleaning_PipelineGH

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nlPOQRdU9HaICvWELs4IXEKma1o75GRb
"""

import pandas as pd
import os
import glob

# Columns we want to keep
columns_to_keep = ['First Name','Last Name','Address Street',
                   'Address City', 'Address Postcode','Vehicle Make','Marital Status']

# ------------------------------
# USER CONFIGURATION
# ------------------------------
# Replace these paths with your own folders
# input_folder: folder containing your original CSVs
# output_folder: folder where cleaned CSVs will be saved
input_folder = '/path/to/your/input/folder/'       # <-- Change this
output_folder = '/path/to/your/output/folder/'     # <-- Change this
log_file = os.path.join(output_folder, 'cleaning_log.csv')

# Create output folder if it doesn't exist
os.makedirs(output_folder, exist_ok=True)

# List to store log info
log_list = []

def clean_and_save(csv_path, save_folder):
    # Read CSV
    df = pd.read_csv(csv_path)

    # Keep only desired columns
    kept_cols = [col for col in columns_to_keep if col in df.columns]
    discarded_cols = [col for col in df.columns if col not in kept_cols]

    new_df = df[kept_cols]

    # Save cleaned CSV
    base_name = os.path.basename(csv_path)
    save_name = f"cleaned_{base_name}"
    file_path = os.path.join(save_folder, save_name)
    new_df.to_csv(file_path, index=False)

    # Add to log
    log_list.append({
        'file': base_name,
        'kept_columns': ', '.join(kept_cols),
        'discarded_columns': ', '.join(discarded_cols),
        'rows': len(df)
    })

    print(f"Cleaned file created: {file_path}")

# Process all CSV files in the input folder
for csv_file in glob.glob(os.path.join(input_folder, '*.csv')):
    clean_and_save(csv_file, output_folder)

# Save log
log_df = pd.DataFrame(log_list)
log_df.to_csv(log_file, index=False)
print(f"Cleaning log saved at: {log_file}")